{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64349d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1641222967519_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-100-77-34-194.ec2.internal:20888/proxy/application_1641222967519_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-100-77-34-172.ec2.internal:8042/node/containerlogs/container_1641222967519_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)"
     ]
    }
   ],
   "source": [
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col,year,month,dayofmonth,to_date,from_unixtime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.sql.functions as sf\n",
    "from datetime import datetime, timezone\n",
    "import pytz\n",
    "from pyspark.sql.functions import md5, concat_ws,sha2\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import col,year,month,dayofmonth,to_date,from_unixtime\n",
    "import boto3\n",
    "client = boto3.client('s3')\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import collect_set, struct,map_concat,expr, split, explode, lit, coalesce, first,explode, explode_outer\n",
    "from pyspark.sql.functions import col,lit,create_map,struct,udf,collect_list\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField, array, MapType\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import unittest\n",
    "import sys\n",
    "import boto3\n",
    "import filecmp\n",
    "import time\n",
    "glue = boto3.client('glue', region_name='us-east-1')\n",
    "client = boto3.client('cloudformation', region_name='us-east-1')\n",
    "athena = boto3.client('athena', region_name='us-east-1')\n",
    "s3 = boto3.client('s3', region_name='us-east-1')\n",
    "# Initiating Glue Context\n",
    "#args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "#sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "#job.init(args['JOB_NAME'], args)\n",
    "#param_jobid = args['JOB_RUN_ID']\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fc536",
   "metadata": {},
   "source": [
    "Checking Existence of Spark Environment Variables\n",
    "Make sure your notebook is loaded using a PySpark Workspace. \n",
    "\n",
    "If you open up a regular Jupyter workspace the following variables might not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1f624053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'sc' not in locals():\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.context import SQLContext\n",
    "    from pyspark.sql.session import SparkSession\n",
    "    \n",
    "    sc = SparkContext()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c23792",
   "metadata": {},
   "source": [
    "Create a utility function to run SQL commands\n",
    "Instead of typing the same python functions repeatedly, we build a small function where you can just pass your query to get results.\n",
    "\n",
    "- Remember we are using Spark SQL in PySpark\n",
    "- We can't run multiple SQL statements in one go (no semi-colon ';' separated SQL statements)\n",
    "- We can run multi-line SQL queries (but still has to be a single statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "978862d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_sql(statement):\n",
    "    try:\n",
    "        result = sqlContext.sql(statement)\n",
    "    except Exception as e:\n",
    "        print(e.desc, '\\n', e.stackTrace)\n",
    "        return\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "59f52c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.context import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a1b3e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  database tableName  isTemporary\n",
      "0              adult         True\n",
      "1             result         True"
     ]
    }
   ],
   "source": [
    "tbls = run_sql('show tables')\n",
    "tbls.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42872e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "run_sql('drop table if exists adult')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32160156",
   "metadata": {},
   "source": [
    "#### Loading the Dataset\n",
    "\n",
    "Below we will use Spark SQL to load in the data and then register it as a Dataframe. \n",
    "So the end result will be a Spark SQL table called adult and a Spark Dataframe called adult_df.\n",
    "\n",
    "This is an example of the flexibility in Spark in that you could do lots of you ETL and data wrangling using either Spark SQL or Dataframes and pyspark. Most of the time it's a case of using whatever you are most comfortable with.\n",
    "\n",
    "When you get more advanced then you might looking the pro's and con's of each and when you might favour one or the other (or operating direclty on RDD's), here is a good article on the issues. For now, no need to overthink it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ab1bc",
   "metadata": {},
   "source": [
    "##### Creating the DataFrame\n",
    "\n",
    "In this section, we will be creating a spark dataframe from the adult dataset which is easier work with when building machine learning models.\n",
    "\n",
    "To get started, first make sure you have already uploaded the adult.data.csv CSV file and it is present in the same directory as the notebook.\n",
    "\n",
    "Once you have done this, please remember to execute the following code to build the dataframe which can also be accessed as a table using spark SQL which we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5e6e8722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "# File location and type\n",
    "file_location_adult = \"s3://mymzbuildtest/adult.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# define the schema based on the dataset dictionary\n",
    "# this is available here: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "schema = StructType([\n",
    "      StructField('age', DoubleType()),\n",
    "      StructField('workclass', StringType()),\n",
    "      StructField('fnlwgt', DoubleType()),\n",
    "      StructField('education', StringType()),\n",
    "      StructField('education_num', DoubleType()),\n",
    "      StructField('marital_status', StringType()),\n",
    "      StructField('occupation', StringType()),\n",
    "      StructField('relationship', StringType()),\n",
    "      StructField('race', StringType()),\n",
    "      StructField('sex', StringType()),\n",
    "      StructField('capital_gain', DoubleType()),\n",
    "      StructField('capital_loss', DoubleType()),\n",
    "      StructField('hours_per_week', DoubleType()),\n",
    "      StructField('native_country', StringType()),\n",
    "      StructField('income', StringType())\n",
    "])\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "adult_df = (spark.read.format(file_type) \n",
    "                    .schema(schema)\n",
    "                    .option(\"inferSchema\", infer_schema) \n",
    "                    .option(\"header\", first_row_is_header) \n",
    "                    .option(\"sep\", delimiter) \n",
    "                    .load(file_location_adult))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4521ba8",
   "metadata": {},
   "source": [
    "#### Viewing the dataframe schemas\n",
    "We can take a look at the schemas of our potential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "366c458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Schema\n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "print('Adult Dataset Schema')\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7c9bfb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  database tableName  isTemporary\n",
      "0              adult         True\n",
      "1             result         True"
     ]
    }
   ],
   "source": [
    "adult_df.registerTempTable(\"adult\")\n",
    "tbls = run_sql('show tables')\n",
    "tbls.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c2ede",
   "metadata": {},
   "source": [
    "### Viewing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "596b7a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age          workclass   ...    native_country  income\n",
      "0  39.0          State-gov   ...     United-States   <=50K\n",
      "1  50.0   Self-emp-not-inc   ...     United-States   <=50K\n",
      "2  38.0            Private   ...     United-States   <=50K\n",
      "3  53.0            Private   ...     United-States   <=50K\n",
      "4  28.0            Private   ...              Cuba   <=50K\n",
      "\n",
      "[5 rows x 15 columns]"
     ]
    }
   ],
   "source": [
    "df = run_sql(\"SELECT * FROM adult LIMIT 5\")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09199764",
   "metadata": {},
   "source": [
    "If you are more comfortable with SQL then as you can see below, its very easy to just get going with writing standard SQL type code to analyse your data, do data wrangling and create new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1147a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            occupation     n      ...        separated_rate  bachelor_rate\n",
      "0       Prof-specialty  4140      ...                  0.02           0.30\n",
      "1         Craft-repair  4099      ...                  0.03           0.21\n",
      "2      Exec-managerial  4066      ...                  0.02           0.20\n",
      "3         Adm-clerical  3770      ...                  0.04           0.42\n",
      "4                Sales  3650      ...                  0.03           0.36\n",
      "5        Other-service  3295      ...                  0.06           0.50\n",
      "6    Machine-op-inspct  2002      ...                  0.04           0.29\n",
      "7                    ?  1843      ...                  0.04           0.42\n",
      "8     Transport-moving  1597      ...                  0.02           0.21\n",
      "9    Handlers-cleaners  1370      ...                  0.03           0.51\n",
      "10     Farming-fishing   994      ...                  0.02           0.29\n",
      "11        Tech-support   928      ...                  0.03           0.36\n",
      "12     Protective-serv   649      ...                  0.02           0.24\n",
      "13     Priv-house-serv   149      ...                  0.08           0.45\n",
      "14        Armed-Forces     9      ...                  0.00           0.67\n",
      "\n",
      "[15 rows x 7 columns]"
     ]
    }
   ],
   "source": [
    "result = run_sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    SUM(1) as n,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) LIKE 'Married-%',1,0)),2) as married_rate,\n",
    "    ROUND(AVG(if(lower(marital_status) LIKE '%widow%',1,0)),2) as widow_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Divorced',1,0)),2) as divorce_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Separated',1,0)),2) as separated_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY n DESC\n",
    "  \"\"\")\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f14109",
   "metadata": {},
   "source": [
    "You can easily register dataframes as a table for Spark SQL too. So this way you can easily move between Dataframes and Spark SQL for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "46ebbd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+------------+----------+------------+--------------+-------------+\n",
      "|      occupation|   n|married_rate|widow_rate|divorce_rate|separated_rate|bachelor_rate|\n",
      "+----------------+----+------------+----------+------------+--------------+-------------+\n",
      "|  Prof-specialty|4140|        0.53|      0.02|        0.13|          0.02|          0.3|\n",
      "|    Craft-repair|4099|        0.64|      0.01|        0.11|          0.03|         0.21|\n",
      "| Exec-managerial|4066|        0.61|      0.02|        0.15|          0.02|          0.2|\n",
      "|    Adm-clerical|3770|        0.28|      0.04|        0.22|          0.04|         0.42|\n",
      "|           Sales|3650|        0.47|      0.03|        0.12|          0.03|         0.36|\n",
      "+----------------+----+------------+----------+------------+--------------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# register the df we just made as a table for spark sql\n",
    "sqlContext.registerDataFrameAsTable(result, \"result\")\n",
    "spark.sql(\"SELECT * FROM result\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6c14a",
   "metadata": {},
   "source": [
    "Writing some spark sql to get the top 'bachelor_rate' by 'education' group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7879dd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|education|bachelor_rate|\n",
      "+---------+-------------+\n",
      "|     12th|         0.54|\n",
      "+---------+-------------+"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    education,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY bachelor_rate DESC\n",
    "  LIMIT 1\n",
    "  \"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78453872",
   "metadata": {},
   "source": [
    "##### Spark DataFrames\n",
    "Below we will create our DataFrame from the SQL table and do some similar analysis as we did with Spark SQL but using the DataFrames API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8855cef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# register a df from the sql df\n",
    "df_adult = spark.table(\"adult\")\n",
    "cols = df_adult.columns # this will be used much later in the notebook, ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "000a4c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# look at df schema\n",
    "df_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "73c9afc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| age|        workclass|  fnlwgt| education|education_num|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|39.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "|38.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# look at the df\n",
    "df_adult.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4a390",
   "metadata": {},
   "source": [
    "As you can see the dataframes api is a bit more verbose then just expressing what you want to do in standard SQL.\n",
    "\n",
    "But some prefer it and might be more used to it, and there could be cases where expressing what you need to do might just be better using the DataFrame API if it is too complicated for a simple SQL expression for example of maybe involves recursion of some type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b2dde",
   "metadata": {},
   "source": [
    "#### Writing some pyspark to get the top 'bachelor_rate' by 'education' group using DataFrame operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "94d9de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|education|bachelor_rate|\n",
      "+---------+-------------+\n",
      "|     12th|         0.54|\n",
      "+---------+-------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, round, desc\n",
    "# wrangle the data a bit\n",
    "df_result = df_adult.select(\n",
    "  df_adult['education'], \n",
    "  # create a yes/no type col on the fly\n",
    "  when( col('marital_status') == ' Never-married' , 1 ).otherwise(0).alias('is_bachelor')\n",
    ")\n",
    "# do grouping (and a round)\n",
    "df_result = df_result.groupBy('education').agg(round(mean('is_bachelor'),2).alias('bachelor_rate'))\n",
    "# do ordering\n",
    "df_result = df_result.orderBy(desc('bachelor_rate'))\n",
    "# show result\n",
    "df_result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf1856",
   "metadata": {},
   "source": [
    "##### Explore & Visualize Data\n",
    "\n",
    "It's very easy to collect() your Spark DataFrame data into a Pandas df and then continue to analyse or plot as you might normally.\n",
    "\n",
    "Obviously if you try to collect() a huge DataFrame then you will run into issues, so usually you would only collect aggregated or sampled data into a Pandas df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "41dbe38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         occupation  plus_50k\n",
      "0   Exec-managerial  0.484014\n",
      "1    Prof-specialty  0.449034\n",
      "2   Protective-serv  0.325116\n",
      "3      Tech-support  0.304957\n",
      "4             Sales  0.269315"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# do some analysis\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    AVG(IF(income = ' >50K',1,0)) as plus_50k\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\")\n",
    "\n",
    "# collect results into a pandas df\n",
    "df_pandas = pd.DataFrame(\n",
    "  result.collect(),\n",
    "  columns=result.schema.names\n",
    ")\n",
    "\n",
    "# look at df\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "96f0a4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        plus_50k\n",
      "count  15.000000\n",
      "mean    0.197354\n",
      "std     0.143994\n",
      "min     0.006711\n",
      "25%     0.107373\n",
      "50%     0.134483\n",
      "75%     0.287136\n",
      "max     0.484014"
     ]
    }
   ],
   "source": [
    "print(df_pandas.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e12a2cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 2 columns):\n",
      "occupation    15 non-null object\n",
      "plus_50k      15 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 320.0+ bytes\n",
      "None"
     ]
    }
   ],
   "source": [
    "print(df_pandas.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d5dae",
   "metadata": {},
   "source": [
    "Here we will just do some very basic plotting to show how you might collect what you are interested in into a Pandas DF and then just plot any way you normally would.\n",
    "\n",
    "For simplicity we are going to use the plotting functionality built into pandas (you could make this a pretty as you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "33529941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i like ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# get simple plot on the pandas data\n",
    "myplot = df_pandas.plot(kind='barh', x='occupation', y='plus_50k')\n",
    "\n",
    "# display the plot (note - display() is a databricks function - \n",
    "# more info on plotting in Databricks is here: https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html)\n",
    "myplot.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e52adfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|               age|    education_num|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             32561|            32561|\n",
      "|   mean| 38.58164675532078| 10.0806793403151|\n",
      "| stddev|13.640432553581356|2.572720332067397|\n",
      "|    min|              17.0|              1.0|\n",
      "|    max|              90.0|             16.0|\n",
      "+-------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "# describe df\n",
    "df_adult.select(df_adult['age'],df_adult['education_num']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac641c",
   "metadata": {},
   "source": [
    "#### ML Pipeline - Logistic Regression vs Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d43e4",
   "metadata": {},
   "source": [
    "Below we will create two Spark ML Pipelines - one that fits a logistic regression and one that fits a random forest. We will then compare the performance of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d5f92c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "72c3943c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf8366fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a5029306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c60b0935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+-----------------+--------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|label|            features| age|        workclass|  fnlwgt| education|education_num|      marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+-----+--------------------+----+-----------------+--------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|  0.0|(100,[4,10,24,32,...|39.0|        State-gov| 77516.0| Bachelors|         13.0|       Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "|  0.0|(100,[1,10,23,31,...|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "|  0.0|(100,[0,8,25,38,4...|38.0|          Private|215646.0|   HS-grad|          9.0|            Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|  0.0|(100,[0,13,23,38,...|53.0|          Private|234721.0|      11th|          7.0|  Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|  0.0|(100,[0,10,23,29,...|28.0|          Private|338409.0| Bachelors|         13.0|  Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "|  0.0|(100,[0,11,23,31,...|37.0|          Private|284582.0|   Masters|         14.0|  Married-civ-spouse|   Exec-managerial|          Wife| White| Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|  0.0|(100,[0,18,28,34,...|49.0|          Private|160187.0|       9th|          5.0| Married-spouse-a...|     Other-service| Not-in-family| Black| Female|         0.0|         0.0|          16.0|       Jamaica| <=50K|\n",
      "|  1.0|(100,[1,8,23,31,4...|52.0| Self-emp-not-inc|209642.0|   HS-grad|          9.0|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          45.0| United-States|  >50K|\n",
      "|  1.0|(100,[0,11,24,29,...|31.0|          Private| 45781.0|   Masters|         14.0|       Never-married|    Prof-specialty| Not-in-family| White| Female|     14084.0|         0.0|          50.0| United-States|  >50K|\n",
      "|  1.0|(100,[0,10,23,31,...|42.0|          Private|159449.0| Bachelors|         13.0|  Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|      5178.0|         0.0|          40.0| United-States|  >50K|\n",
      "+-----+--------------------+----+-----------------+--------+----------+-------------+--------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(df_adult)\n",
    "dataset = pipelineModel.transform(df_adult)\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8954d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22838\n",
      "9723"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "25a31cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate in the training data is 0.23933794552938087"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# get the rate of the positive outcome from the training data to use as a threshold in the model\n",
    "training_data_positive_rate = trainingData.select(avg(trainingData['label'])).collect()[0][0] \n",
    "\n",
    "print(\"Positive rate in the training data is {}\".format(training_data_positive_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8babb0f",
   "metadata": {},
   "source": [
    "#### Logistic Regression - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "544bca73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold based on model performance on training data is 0.3501591272080348"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# set threshold for the probability above which to predict a 1\n",
    "lr.setThreshold(training_data_positive_rate)\n",
    "# lr.setThreshold(0.5) # could use this if knew you had balanced data\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# get training summary used for eval metrics and other params\n",
    "lrTrainingSummary = lrModel.summary\n",
    "\n",
    "# Find the best model threshold if you would like to use that instead of the empirical positve rate\n",
    "fMeasure = lrTrainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "lrBestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "  \n",
    "print(\"Best threshold based on model performance on training data is {}\".format(lrBestThreshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dc998",
   "metadata": {},
   "source": [
    "### GBM - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a96b8",
   "metadata": {},
   "source": [
    "Train a GBTClassifier on the training data, call the trained model 'gbModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2cff1ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "gb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "gbModel = gb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ca26e",
   "metadata": {},
   "source": [
    "Logistic Regression - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4b944082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       1.0|[0.69234532795194...|\n",
      "|  0.0|       1.0|[0.62115531452964...|\n",
      "|  0.0|       1.0|[0.65845294177529...|\n",
      "|  0.0|       1.0|[0.65826620022842...|\n",
      "|  0.0|       1.0|[0.61503423805722...|\n",
      "|  0.0|       1.0|[0.53986082134083...|\n",
      "|  0.0|       1.0|[0.60044732389245...|\n",
      "|  0.0|       1.0|[0.58986249819721...|\n",
      "|  0.0|       1.0|[0.58241874548816...|\n",
      "|  0.0|       1.0|[0.59201535021361...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# make predictions on test data\n",
    "lrPredictions = lrModel.transform(testData)\n",
    "\n",
    "# display predictions\n",
    "(lrPredictions.select(\"label\", \"prediction\", \"probability\")).show(10)\n",
    "#display(lrPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4c026",
   "metadata": {},
   "source": [
    "GBM - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386a45a",
   "metadata": {},
   "source": [
    "Get predictions on the test data for your GBTClassifier. Call the predictions df 'gbPredictions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "367c84a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|label|            features| age|workclass|  fnlwgt|education|education_num|     marital_status|     occupation|relationship|  race|  sex|capital_gain|capital_loss|hours_per_week|native_country|income|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|  0.0|(100,[0,8,23,29,4...|26.0|  Private| 58426.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.74749350368397...|[0.81682561551147...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|30.0|  Private| 83253.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          55.0| United-States| <=50K|[0.61855876232599...|[0.77506187921223...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|31.0|  Private| 62374.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.61855876232599...|[0.77506187921223...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|32.0|  Private| 32732.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.58403224681682...|[0.76279498568528...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|39.0|  Private|181705.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          45.0| United-States| <=50K|[0.16253267494394...|[0.58055821828003...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|47.0|  Private| 76612.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.19291167389702...|[0.59527685227879...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|50.0|  Private| 81548.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.26075848712073...|[0.62750241715382...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|51.0|  Private| 95469.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.26075848712073...|[0.62750241715382...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|60.0|  Private|198727.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.43123143178833...|[0.70317496016640...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|61.0|  Private|128230.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.43123143178833...|[0.70317496016640...|       0.0|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# make predictions on test data\n",
    "gbPredictions = gbModel.transform(testData)\n",
    "\n",
    "(gbPredictions).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cc7d3",
   "metadata": {},
   "source": [
    "Logistic Regression - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a4697166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    print(\"auc = {}\".format(auc))\n",
    "    print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "    predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "    binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "    print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "    print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "    print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "    print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  # F1\n",
    "    print(\"F1 = {}\".format(multi_metrics.weightedFMeasure(1.0)))\n",
    "  # Precision\n",
    "    print(\"Precision = {}\".format(multi_metrics.weightedPrecision))\n",
    "  # Recall\n",
    "    print(\"Recall = {}\".format(multi_metrics.weightedRecall))\n",
    "  # FPR\n",
    "    print(\"FPR = {}\".format(multi_metrics.weightedFalsePositiveRate))\n",
    "  # TPR\n",
    "    print(\"TPR = {}\".format(multi_metrics.weightedTruePositiveRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dce34",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "For each model you can run the below comand to see its params and a brief explanation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ffda2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5, current: 0.23933794552938087)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c0db8050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 10)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 3504127614838123891)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)"
     ]
    }
   ],
   "source": [
    "print(gb.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc770fc",
   "metadata": {},
   "source": [
    "Logisitic Regression - Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d8f67598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lrParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [2, 5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b00e87",
   "metadata": {},
   "source": [
    "GBM - Param Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be50318",
   "metadata": {},
   "source": [
    "Build out a param grid for the gb model, call it 'gbParamGrid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7f823c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "gbParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(gb.maxDepth, [5, 10])\n",
    "             .addGrid(gb.maxIter, [2, 5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649645c",
   "metadata": {},
   "source": [
    "Logistic Regression - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bd9fe584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up an evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Create CrossValidator\n",
    "lrCv = CrossValidator(estimator=lr, estimatorParamMaps=lrParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "lrCvModel = lrCv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2f7d53aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.0\n",
      "5"
     ]
    }
   ],
   "source": [
    "# below approach to getting at the best params from the best cv model taken from:\n",
    "# https://stackoverflow.com/a/46353730/1919374\n",
    "\n",
    "# look at best params from the CV\n",
    "print(lrCvModel.bestModel._java_obj.getRegParam())\n",
    "print(lrCvModel.bestModel._java_obj.getElasticNetParam())\n",
    "print(lrCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8dfe8",
   "metadata": {},
   "source": [
    "GBM - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2ec31",
   "metadata": {},
   "source": [
    "- Perform cross validation of params on your 'gb' model.\n",
    "- Print out the best params you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7fa5d971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CrossValidator\n",
    "gbCv = CrossValidator(estimator=gb, estimatorParamMaps=gbParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "gbCvModel = gbCv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8e7a342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5"
     ]
    }
   ],
   "source": [
    "# look at best params from the CV\n",
    "print(gbCvModel.bestModel._java_obj.getMaxDepth())\n",
    "print(gbCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cc150",
   "metadata": {},
   "source": [
    "Logistic Regression - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "586ee8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|label|            features| age|workclass|  fnlwgt|education|education_num|     marital_status|     occupation|relationship|  race|  sex|capital_gain|capital_loss|hours_per_week|native_country|income|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|  0.0|(100,[0,8,23,29,4...|26.0|  Private| 58426.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.24869280328235...|[0.56185472911394...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|30.0|  Private| 83253.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          55.0| United-States| <=50K|[0.22764466247731...|[0.55666666130698...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|31.0|  Private| 62374.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.23099943840427...|[0.55749442459509...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|32.0|  Private| 32732.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.20460300867728...|[0.55097305494903...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|39.0|  Private|181705.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          45.0| United-States| <=50K|[0.31074633033305...|[0.57706742143929...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|47.0|  Private| 76612.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.17558576631787...|[0.54378400960233...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|50.0|  Private| 81548.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.21323631793825...|[0.55310799797174...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|51.0|  Private| 95469.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.21956089063506...|[0.55467077282260...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|60.0|  Private|198727.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.30627243700351...|[0.57597514550730...|       1.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|61.0|  Private|128230.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.24918903544206...|[0.56197688481390...|       1.0|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "lrCvPredictions = lrCvModel.transform(testData)\n",
    "\n",
    "(lrCvPredictions).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6a94b",
   "metadata": {},
   "source": [
    "GBM - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2d2ee27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|label|            features| age|workclass|  fnlwgt|education|education_num|     marital_status|     occupation|relationship|  race|  sex|capital_gain|capital_loss|hours_per_week|native_country|income|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "|  0.0|(100,[0,8,23,29,4...|26.0|  Private| 58426.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.66410216901839...|[0.79054345612166...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|30.0|  Private| 83253.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          55.0| United-States| <=50K|[0.60875968361924...|[0.77162670954357...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|31.0|  Private| 62374.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.60875968361924...|[0.77162670954357...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|32.0|  Private| 32732.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.60875968361924...|[0.77162670954357...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|39.0|  Private|181705.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          45.0| United-States| <=50K|[0.21763911069944...|[0.60713335209765...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|47.0|  Private| 76612.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          50.0| United-States| <=50K|[0.21763911069944...|[0.60713335209765...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|50.0|  Private| 81548.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.21763911069944...|[0.60713335209765...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|51.0|  Private| 95469.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          40.0| United-States| <=50K|[0.21763911069944...|[0.60713335209765...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|60.0|  Private|198727.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.33390476308065...|[0.66101250208196...|       0.0|\n",
      "|  0.0|(100,[0,8,23,29,4...|61.0|  Private|128230.0|  HS-grad|          9.0| Married-civ-spouse| Prof-specialty|     Husband| White| Male|         0.0|         0.0|          30.0| United-States| <=50K|[0.33390476308065...|[0.66101250208196...|       0.0|\n",
      "+-----+--------------------+----+---------+--------+---------+-------------+-------------------+---------------+------------+------+-----+------------+------------+--------------+--------------+------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "gbCvPredictions = gbCvModel.transform(testData)\n",
    "\n",
    "(gbCvPredictions).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f354be0",
   "metadata": {},
   "source": [
    "Logistic Regression - Model Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "52554103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  -1.2494013222176612"
     ]
    }
   ],
   "source": [
    "print('Model Intercept: ', lrCvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3b4cf0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Feature Weight|\n",
      "+--------------------+\n",
      "|-0.22287498646428372|\n",
      "|-0.35326178327212027|\n",
      "|-0.13932588909455518|\n",
      "|-0.46972707751637405|\n",
      "|-0.25764729980659923|\n",
      "| 0.43106088678205384|\n",
      "|  0.4049563196835919|\n",
      "| -1.1587404641846368|\n",
      "|-0.43938998811469526|\n",
      "|-0.19390384395684315|\n",
      "| 0.43943734511985366|\n",
      "|  0.7977426931125261|\n",
      "|-0.04370489071461...|\n",
      "| -0.6521415604868107|\n",
      "|-0.06631870206048067|\n",
      "|  -0.668354499486831|\n",
      "| -0.8885122319716758|\n",
      "|  1.2048122786164326|\n",
      "| -0.8180370321560908|\n",
      "| -0.5400407972184569|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "lrWeights = lrCvModel.bestModel.coefficients\n",
    "lrWeights = [(float(w),) for w in lrWeights]  # convert numpy type to float, and to tuple\n",
    "lrWeightsDF = sqlContext.createDataFrame(lrWeights, [\"Feature Weight\"])\n",
    "(lrWeightsDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb105693",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723f3cf",
   "metadata": {},
   "source": [
    "Print out a table of feature_name and feature_coefficient from the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f0653901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from: https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name\n",
    "from itertools import chain\n",
    "\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*lrCvPredictions\n",
    "        .schema[lrCvModel.bestModel.summary.featuresCol]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c3ca9e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  feature_name  feature_importance\n",
      "23  marital_statusclassVec_ Married-civ-spouse            0.252280\n",
      "97                                capital_gain            0.127076\n",
      "94                                         age            0.122879\n",
      "96                               education_num            0.115945\n",
      "31         occupationclassVec_ Exec-managerial            0.092742\n",
      "98                                capital_loss            0.076970\n",
      "99                              hours_per_week            0.064418\n",
      "1          workclassclassVec_ Self-emp-not-inc            0.022999\n",
      "10                educationclassVec_ Bachelors            0.018529\n",
      "40            occupationclassVec_ Tech-support            0.015693\n",
      "29          occupationclassVec_ Prof-specialty            0.015373\n",
      "52                           sexclassVec_ Male            0.013566\n",
      "39         occupationclassVec_ Farming-fishing            0.013390\n",
      "34           occupationclassVec_ Other-service            0.010530\n",
      "6               workclassclassVec_ Federal-gov            0.008769\n",
      "43               relationshipclassVec_ Husband            0.006937\n",
      "8                   educationclassVec_ HS-grad            0.005590\n",
      "55                   native_countryclassVec_ ?            0.003166\n",
      "30            occupationclassVec_ Craft-repair            0.002953\n",
      "48                         raceclassVec_ White            0.002263\n",
      "14               educationclassVec_ Assoc-acdm            0.002250\n",
      "95                                      fnlwgt            0.001643\n",
      "11                  educationclassVec_ Masters            0.001148\n",
      "84                native_countryclassVec_ Hong            0.000917\n",
      "44         relationshipclassVec_ Not-in-family            0.000851\n",
      "17              educationclassVec_ Prof-school            0.000487\n",
      "33                   occupationclassVec_ Sales            0.000397\n",
      "76                native_countryclassVec_ Iran            0.000112\n",
      "0                   workclassclassVec_ Private            0.000079\n",
      "67               native_countryclassVec_ Italy            0.000049\n",
      "..                                         ...                 ...\n",
      "41         occupationclassVec_ Protective-serv            0.000000\n",
      "21                  educationclassVec_ 5th-6th            0.000000\n",
      "22                  educationclassVec_ 1st-4th            0.000000\n",
      "37        occupationclassVec_ Transport-moving            0.000000\n",
      "46             relationshipclassVec_ Unmarried            0.000000\n",
      "36                       occupationclassVec_ ?            0.000000\n",
      "35       occupationclassVec_ Machine-op-inspct            0.000000\n",
      "24       marital_statusclassVec_ Never-married            0.000000\n",
      "25            marital_statusclassVec_ Divorced            0.000000\n",
      "32            occupationclassVec_ Adm-clerical            0.000000\n",
      "26           marital_statusclassVec_ Separated            0.000000\n",
      "45             relationshipclassVec_ Own-child            0.000000\n",
      "47                  relationshipclassVec_ Wife            0.000000\n",
      "63             native_countryclassVec_ England            0.000000\n",
      "56         native_countryclassVec_ Philippines            0.000000\n",
      "62                native_countryclassVec_ Cuba            0.000000\n",
      "61               native_countryclassVec_ India            0.000000\n",
      "60         native_countryclassVec_ El-Salvador            0.000000\n",
      "59         native_countryclassVec_ Puerto-Rico            0.000000\n",
      "58              native_countryclassVec_ Canada            0.000000\n",
      "57             native_countryclassVec_ Germany            0.000000\n",
      "13                     educationclassVec_ 11th            0.000000\n",
      "18                      educationclassVec_ 9th            0.000000\n",
      "54              native_countryclassVec_ Mexico            0.000000\n",
      "53       native_countryclassVec_ United-States            0.000000\n",
      "15                     educationclassVec_ 10th            0.000000\n",
      "51            raceclassVec_ Amer-Indian-Eskimo            0.000000\n",
      "16                  educationclassVec_ 7th-8th            0.000000\n",
      "49                         raceclassVec_ Black            0.000000\n",
      "50            raceclassVec_ Asian-Pac-Islander            0.000000\n",
      "\n",
      "[100 rows x 2 columns]"
     ]
    }
   ],
   "source": [
    "gbCvFeatureImportance = pd.DataFrame([(name, gbCvModel.bestModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3f389",
   "metadata": {},
   "source": [
    "Build and train a RandomForestClassifier and print out a table of feature importances from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "93332458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     feature_name  feature_importance\n",
      "97                                   capital_gain            0.144650\n",
      "96                                  education_num            0.143814\n",
      "43                  relationshipclassVec_ Husband            0.114401\n",
      "23     marital_statusclassVec_ Married-civ-spouse            0.092665\n",
      "94                                            age            0.075207\n",
      "44            relationshipclassVec_ Not-in-family            0.057966\n",
      "10                   educationclassVec_ Bachelors            0.045989\n",
      "99                                 hours_per_week            0.044771\n",
      "98                                   capital_loss            0.041329\n",
      "45                relationshipclassVec_ Own-child            0.034394\n",
      "31            occupationclassVec_ Exec-managerial            0.032984\n",
      "29             occupationclassVec_ Prof-specialty            0.031772\n",
      "52                              sexclassVec_ Male            0.027105\n",
      "24          marital_statusclassVec_ Never-married            0.024133\n",
      "11                     educationclassVec_ Masters            0.013786\n",
      "47                     relationshipclassVec_ Wife            0.011791\n",
      "25               marital_statusclassVec_ Divorced            0.010536\n",
      "46                relationshipclassVec_ Unmarried            0.010305\n",
      "17                 educationclassVec_ Prof-school            0.009313\n",
      "8                      educationclassVec_ HS-grad            0.007071\n",
      "34              occupationclassVec_ Other-service            0.006592\n",
      "49                            raceclassVec_ Black            0.003992\n",
      "38          occupationclassVec_ Handlers-cleaners            0.003182\n",
      "20                   educationclassVec_ Doctorate            0.002329\n",
      "54                 native_countryclassVec_ Mexico            0.001382\n",
      "13                        educationclassVec_ 11th            0.001158\n",
      "21                     educationclassVec_ 5th-6th            0.000892\n",
      "32               occupationclassVec_ Adm-clerical            0.000865\n",
      "37           occupationclassVec_ Transport-moving            0.000856\n",
      "3                            workclassclassVec_ ?            0.000708\n",
      "..                                            ...                 ...\n",
      "59            native_countryclassVec_ Puerto-Rico            0.000000\n",
      "91               native_countryclassVec_ Honduras            0.000000\n",
      "92                native_countryclassVec_ Hungary            0.000000\n",
      "93               native_countryclassVec_ Scotland            0.000000\n",
      "16                     educationclassVec_ 7th-8th            0.000000\n",
      "12                   educationclassVec_ Assoc-voc            0.000000\n",
      "7                  workclassclassVec_ Without-pay            0.000000\n",
      "2                    workclassclassVec_ Local-gov            0.000000\n",
      "80                 native_countryclassVec_ France            0.000000\n",
      "79                   native_countryclassVec_ Peru            0.000000\n",
      "78              native_countryclassVec_ Nicaragua            0.000000\n",
      "77               native_countryclassVec_ Portugal            0.000000\n",
      "42            occupationclassVec_ Priv-house-serv            0.000000\n",
      "41            occupationclassVec_ Protective-serv            0.000000\n",
      "30               occupationclassVec_ Craft-repair            0.000000\n",
      "63                native_countryclassVec_ England            0.000000\n",
      "64                native_countryclassVec_ Jamaica            0.000000\n",
      "65                  native_countryclassVec_ South            0.000000\n",
      "66                  native_countryclassVec_ China            0.000000\n",
      "28  marital_statusclassVec_ Married-spouse-absent            0.000000\n",
      "68     native_countryclassVec_ Dominican-Republic            0.000000\n",
      "69                native_countryclassVec_ Vietnam            0.000000\n",
      "70              native_countryclassVec_ Guatemala            0.000000\n",
      "71                  native_countryclassVec_ Japan            0.000000\n",
      "72                 native_countryclassVec_ Poland            0.000000\n",
      "22                     educationclassVec_ 1st-4th            0.000000\n",
      "19                        educationclassVec_ 12th            0.000000\n",
      "75                  native_countryclassVec_ Haiti            0.000000\n",
      "18                         educationclassVec_ 9th            0.000000\n",
      "50               raceclassVec_ Asian-Pac-Islander            0.000000\n",
      "\n",
      "[100 rows x 2 columns]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "rfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(rfFeatureImportance.sort_values(by=['feature_importance'],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98b14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
